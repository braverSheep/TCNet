## TCNet: Hybrid CNN-Transformer Framework with Dynamic Feature Fusion for Enhanced Passport Background Texture Classification
### Maoqin Tian, Lin Tang, JiaFeng Xu, Yibo Zhang, Yong Yang, Lingpei Zeng, Eryang Chen, Yuanlun Xie
---
This paper is being submitted to the journal â€œThe Visual Computerâ€ for publication
===
Verification of passport authenticity is particularly important for global security and border management. The recognition of passport background textures is one of the key technologies for determining the authenticity of passports. This work proposes a complementary CNN and Transformer framework, namely TCNet, for the recognition of passport background textures. The framework uses CNN to extract local features of the background texture of the passport, uses Transformers to model the global features of the texture, and finally uses a dynamic weighting module (DWM) to fuse the features of the two, ultimately improving the performance of classifying the background texture of the passport.

## Pytorch
We used Pytorch for the implementation of our code

## Requirements
1. Python 3.11
2. torch 2.3.1+cu121
3. torchvision 0.18.1+cu121
4. cuda 12.1

Only a basic environment setup is needed to implement our method.

## ğŸ“ Folder structure

Download all files from our repo. The following shows the basic folder structure.

```text
data
â”œâ”€â”€ test_data
â”œâ”€â”€ train_data
lowlight_test.py      # testing code
lowlight_train.py     # training code
model.py              # the proposed network
dataloader.py
Metric
â”œâ”€â”€ akh_brisque.py
â”œâ”€â”€ BRISQUE.py
â”œâ”€â”€ eff.py
â”œâ”€â”€ ref.py
â”œâ”€â”€ SPAQ.py
model_parameter
â”œâ”€â”€ Epoch99.pth       # A pre-trained model (Epoch99.pth)
snapshots_epochs
```



## Dataset
A passport backing dataset with four main types of backing: 'Transformed design', 'Laser printing', 'Lithographic printing' and 'Inkjet printing', with labels assigned as 0, 1, 2, and 3, respectively, is developed . In particular, 'Transformed design' and 'lithographic printing' are the shading expressions of real passports, and the remaining two are the shading expressions of fake passports. In our study, the dataset was split into a training set and a test set in an 8:2 ratio. <br>
To enhance the diversity and richness of the dataset, data augmentation techniques were applied, improving the model's generalization ability. We augmented the captured images with random horizontal flips, vertical flips, random rotations, color dithering, random affine transformations, and perspective transformations, which increased the number of samples per category in the training set to 1000, and 250 samples per category in the test set. The dataset can be obtained from website :[passport backing dataset](https://pan.baidu.com/s/1pBBza-5w_56DTDZlO3sATQ?pwd=ufxt) <br>

If you wish to use this dataset, please cite the following:<br>
@article{xu2024pbnet,<br>
  title={PBNet: Combining Transformer and CNN in Passport Background Texture Printing Image Classification},<br>
  author={Xu, Jiafeng and Jia, Dawei and Lin, Zhizhe and Zhou, Teng and Wu, Jie and Tang, Lin},<br>
  journal={Electronics},<br>
  volume={13},<br>
  number={21},<br>
  pages={4160},<br>
  year={2024},<br>
  publisher={MDPI}<br>
}

## Contact
If you have any questions, please contact Yuanlun Xie at:[fengyuxiexie@163.com](fengyuxiexie@163.com)
